{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6ZoKGUdEDCC",
        "outputId": "04707c14-8c9d-4fa8-96bd-cfdf612fbf1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LiB4tdy9FPd_"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"Lab12\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfTsWXbqEGBv",
        "outputId": "f08bb146-1f15-49e7-db1f-5eff36a24009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pearson correlation matrix:\n",
            "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
            "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
            "             [       nan,        nan, 1.        ,        nan],\n",
            "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
            "Spearman correlation matrix:\n",
            "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
            "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
            "             [       nan,        nan, 1.        ,        nan],\n",
            "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.linalg import Vectors as vectors\n",
        "from pyspark.ml.stat import Correlation as correlation\n",
        "\n",
        "data= [(vectors.sparse(4, [(0, 1.0), (3, -2.0)]),), (vectors.dense ([4.0, 5.0, 0.0, 3.0]),), (vectors.dense ([6.0, 7.0, 0.0, 8.0]),), (vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
        "\n",
        "df =spark.createDataFrame(data,[\"features\"])\n",
        "\n",
        "r1 = correlation.corr (df, \"features\").head()\n",
        "\n",
        "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
        "\n",
        "r2= correlation.corr(df, \"features\", \"spearman\").head()\n",
        "\n",
        "print(\"Spearman correlation matrix:\\n\"+ str(r2[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peFqOqC2GDI8",
        "outputId": "2544edc7-d525-4a40-ba3c-6c11efaa6f24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pValues: [0.6872892787909721,0.6822703303362126]\n",
            "degreesOfFreedom: [2, 3]\n",
            "statistics: [0.75,1.5]\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import ChiSquareTest\n",
        "\n",
        "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
        "        (0.0, Vectors.dense(1.5, 20.0)),\n",
        "        (1.0, Vectors.dense(1.5, 30.0)),\n",
        "        (0.0, Vectors.dense(3.5, 30.0)),\n",
        "        (0.0, Vectors.dense(3.5, 40.0)),\n",
        "        (1.0, Vectors.dense(3.5, 40.0))]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
        "\n",
        "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
        "\n",
        "print(\"pValues: \" + str(r.pValues))\n",
        "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
        "print(\"statistics: \" + str(r.statistics))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "P3pFpVG_KhrB"
      },
      "outputs": [],
      "source": [
        "spark.stop()\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Create a SparkContext object\n",
        "sc = SparkContext(\"local\", \"example\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URRQfT7vKCCO",
        "outputId": "962bce3e-4115-4054-ddba-7516e4c8c09a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------+\n",
            "|aggregate_metrics(features, weight)|\n",
            "+-----------------------------------+\n",
            "|{[1.0,1.0,1.0], 1}                 |\n",
            "+-----------------------------------+\n",
            "\n",
            "+--------------------------------+\n",
            "|aggregate_metrics(features, 1.0)|\n",
            "+--------------------------------+\n",
            "|{[1.0,1.5,2.0], 2}              |\n",
            "+--------------------------------+\n",
            "\n",
            "+--------------+\n",
            "|mean(features)|\n",
            "+--------------+\n",
            "|[1.0,1.0,1.0] |\n",
            "+--------------+\n",
            "\n",
            "+--------------+\n",
            "|mean(features)|\n",
            "+--------------+\n",
            "|[1.0,1.5,2.0] |\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.stat import Summarizer\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create an RDD\n",
        "rdd = sc.parallelize([\n",
        "    (1.0, Vectors.dense(1.0, 1.0, 1.0)),\n",
        "    (0.0, Vectors.dense(1.0, 2.0, 3.0))\n",
        "])\n",
        "\n",
        "# Convert RDD to DataFrame\n",
        "df = spark.createDataFrame(rdd, [\"weight\", \"features\"])\n",
        "\n",
        "# Create summarizer for multiple metrics \"mean\" and \"count\"\n",
        "summarizer = Summarizer.metrics(\"mean\", \"count\")\n",
        "\n",
        "# Compute statistics for multiple metrics with weight\n",
        "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
        "\n",
        "# Compute statistics for multiple metrics without weight\n",
        "df.select(summarizer.summary(df.features)).show(truncate=False)\n",
        "\n",
        "# Compute statistics for single metric \"mean\" with weight\n",
        "df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
        "\n",
        "# Compute statistics for single metric \"mean\" without weight\n",
        "df.select(Summarizer.mean(df.features)).show(truncate=False)\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
