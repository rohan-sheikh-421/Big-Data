{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POelmz06kRJN",
        "outputId": "8068c87d-34cf-4705-842e-9746cf32b270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=4b9780870db051fde985fd4a293a9a9764753e8075647007ef6d0082a7d2fa5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "id": "uSi83uTikNVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ76XSQ1ikXR"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"emp_join\").getOrCreate()\n",
        "\n",
        "# Read the CSV files into DataFrames\n",
        "rdd1 = spark.read.csv(\"emp1.txt\", header=True, inferSchema=True)\n",
        "rdd2 = spark.read.csv(\"emp2.txt\", header=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1"
      ],
      "metadata": {
        "id": "1w0bVpGqm8I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform an inner join on emp_id\n",
        "result = rdd1.join(rdd2, on=[\"emp_id\"])\n",
        "\n",
        "# Display the result\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUySsw6dm42l",
        "outputId": "f8a6e06e-8c5a-4398-934e-36af2f36113d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+--------------------+----------+--------------+\n",
            "|emp_id|emp_name|            emp_dept|emp_salary|emp_experience|\n",
            "+------+--------+--------------------+----------+--------------+\n",
            "|     1|    John|   Digital Marketing|     50000|             2|\n",
            "|     2|   Alice|Software Development|     65000|             5|\n",
            "|     3|     Bob|        Data Science|     70000|             3|\n",
            "|     4| Charlie|     Human Resources|     55000|             1|\n",
            "|     5|     Eve|             Finance|     60000|             4|\n",
            "|     6| Michael|Software Development|     80000|             6|\n",
            "|     7|  Olivia|        Data Science|     90000|             3|\n",
            "|     8|  Sophia|             Finance|     75000|             2|\n",
            "|     9|    Liam|        Data Science|     55000|             4|\n",
            "|    10|   Emily|Software Development|     70000|             5|\n",
            "|    11|   James|             Finance|     60000|             3|\n",
            "|    12|Benjamin|   Digital Marketing|     45000|             1|\n",
            "|    13|    Emma|Software Development|     85000|             7|\n",
            "|    14|     Ava|        Data Science|     75000|             4|\n",
            "|    15|Isabella|     Human Resources|     70000|             2|\n",
            "|    16|   Lucas|             Finance|     80000|             3|\n",
            "|    17|   Avery|Software Development|     55000|             1|\n",
            "|    18|   Grace|        Data Science|     65000|             4|\n",
            "|    19|   Ethan|     Human Resources|     60000|             2|\n",
            "|    20|Scarlett|Software Development|     70000|             3|\n",
            "+------+--------+--------------------+----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2"
      ],
      "metadata": {
        "id": "1UdovbZwm_OM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a left outer join on emp_id\n",
        "result = rdd1.join(rdd2, on=[\"emp_id\"], how=\"leftouter\")\n",
        "\n",
        "# Display the result\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GhuZfqskLe2",
        "outputId": "f57a92bf-723b-4dc0-8e36-d45c909843ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+--------------------+----------+--------------+\n",
            "|emp_id|emp_name|            emp_dept|emp_salary|emp_experience|\n",
            "+------+--------+--------------------+----------+--------------+\n",
            "|     1|    John|   Digital Marketing|     50000|             2|\n",
            "|     2|   Alice|Software Development|     65000|             5|\n",
            "|     3|     Bob|        Data Science|     70000|             3|\n",
            "|     4| Charlie|     Human Resources|     55000|             1|\n",
            "|     5|     Eve|             Finance|     60000|             4|\n",
            "|     6| Michael|Software Development|     80000|             6|\n",
            "|     7|  Olivia|        Data Science|     90000|             3|\n",
            "|     8|  Sophia|             Finance|     75000|             2|\n",
            "|     9|    Liam|        Data Science|     55000|             4|\n",
            "|    10|   Emily|Software Development|     70000|             5|\n",
            "|    11|   James|             Finance|     60000|             3|\n",
            "|    12|Benjamin|   Digital Marketing|     45000|             1|\n",
            "|    13|    Emma|Software Development|     85000|             7|\n",
            "|    14|     Ava|        Data Science|     75000|             4|\n",
            "|    15|Isabella|     Human Resources|     70000|             2|\n",
            "|    16|   Lucas|             Finance|     80000|             3|\n",
            "|    17|   Avery|Software Development|     55000|             1|\n",
            "|    18|   Grace|        Data Science|     65000|             4|\n",
            "|    19|   Ethan|     Human Resources|     60000|             2|\n",
            "|    20|Scarlett|Software Development|     70000|             3|\n",
            "+------+--------+--------------------+----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3"
      ],
      "metadata": {
        "id": "dyILDO20nAsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiply emp_salary with emp_experience\n",
        "result = rdd1.join(rdd2, on=[\"emp_id\"])\n",
        "result = result.select(rdd1[\"emp_id\"], rdd1[\"emp_name\"], rdd1[\"emp_dept\"], rdd2[\"emp_salary\"] * rdd2[\"emp_experience\"].cast(\"double\").alias(\"total_salary\"))\n",
        "\n",
        "# Display the result\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9NUZT6Kl5Jm",
        "outputId": "ed0d3fb6-544e-4a92-f226-a5dee0462feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+--------------------+-------------------------------------------------------------+\n",
            "|emp_id|emp_name|            emp_dept|(emp_salary * CAST(emp_experience AS DOUBLE) AS total_salary)|\n",
            "+------+--------+--------------------+-------------------------------------------------------------+\n",
            "|     1|    John|   Digital Marketing|                                                     100000.0|\n",
            "|     2|   Alice|Software Development|                                                     325000.0|\n",
            "|     3|     Bob|        Data Science|                                                     210000.0|\n",
            "|     4| Charlie|     Human Resources|                                                      55000.0|\n",
            "|     5|     Eve|             Finance|                                                     240000.0|\n",
            "|     6| Michael|Software Development|                                                     480000.0|\n",
            "|     7|  Olivia|        Data Science|                                                     270000.0|\n",
            "|     8|  Sophia|             Finance|                                                     150000.0|\n",
            "|     9|    Liam|        Data Science|                                                     220000.0|\n",
            "|    10|   Emily|Software Development|                                                     350000.0|\n",
            "|    11|   James|             Finance|                                                     180000.0|\n",
            "|    12|Benjamin|   Digital Marketing|                                                      45000.0|\n",
            "|    13|    Emma|Software Development|                                                     595000.0|\n",
            "|    14|     Ava|        Data Science|                                                     300000.0|\n",
            "|    15|Isabella|     Human Resources|                                                     140000.0|\n",
            "|    16|   Lucas|             Finance|                                                     240000.0|\n",
            "|    17|   Avery|Software Development|                                                      55000.0|\n",
            "|    18|   Grace|        Data Science|                                                     260000.0|\n",
            "|    19|   Ethan|     Human Resources|                                                     120000.0|\n",
            "|    20|Scarlett|Software Development|                                                     210000.0|\n",
            "+------+--------+--------------------+-------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4"
      ],
      "metadata": {
        "id": "K5tEft9lnC38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.broadcast import Broadcast\n",
        "\n",
        "# SparkSession\n",
        "spark = SparkSession.builder.appName(\"broadcast_join\").getOrCreate()\n",
        "\n",
        "# Read the CSV files into DataFrames\n",
        "rdd1 = spark.read.csv(\"emp1.txt\", header=True, inferSchema=True)\n",
        "rdd2 = spark.read.csv(\"emp2.txt\", header=True, inferSchema=True)\n",
        "\n",
        "# Broadcast the first RDD\n",
        "broadcast_rdd1 = spark.sparkContext.broadcast(rdd1.rdd.map(lambda x: (x[\"emp_id\"], x)).collectAsMap())\n",
        "\n",
        "# Perform a broadcast join on the emp_id column\n",
        "result = rdd2.rdd.map(lambda x: (x[\"emp_id\"], x[\"emp_salary\"], broadcast_rdd1.value.get(x[\"emp_id\"], {\"emp_name\": None, \"emp_dept\": None})[\"emp_name\"], broadcast_rdd1.value.get(x[\"emp_id\"], {\"emp_name\": None, \"emp_dept\": None})[\"emp_dept\"])).toDF([\"emp_id\", \"emp_salary\", \"emp_name\", \"emp_dept\"])\n",
        "\n",
        "# Display the result\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzxPeuXbl8Z2",
        "outputId": "3b3f4e93-454f-4e13-c805-170086173478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+--------+--------------------+\n",
            "|emp_id|emp_salary|emp_name|            emp_dept|\n",
            "+------+----------+--------+--------------------+\n",
            "|     1|     50000|    John|   Digital Marketing|\n",
            "|     2|     65000|   Alice|Software Development|\n",
            "|     3|     70000|     Bob|        Data Science|\n",
            "|     4|     55000| Charlie|     Human Resources|\n",
            "|     5|     60000|     Eve|             Finance|\n",
            "|     6|     80000| Michael|Software Development|\n",
            "|     7|     90000|  Olivia|        Data Science|\n",
            "|     8|     75000|  Sophia|             Finance|\n",
            "|     9|     55000|    Liam|        Data Science|\n",
            "|    10|     70000|   Emily|Software Development|\n",
            "|    11|     60000|   James|             Finance|\n",
            "|    12|     45000|Benjamin|   Digital Marketing|\n",
            "|    13|     85000|    Emma|Software Development|\n",
            "|    14|     75000|     Ava|        Data Science|\n",
            "|    15|     70000|Isabella|     Human Resources|\n",
            "|    16|     80000|   Lucas|             Finance|\n",
            "|    17|     55000|   Avery|Software Development|\n",
            "|    18|     65000|   Grace|        Data Science|\n",
            "|    19|     60000|   Ethan|     Human Resources|\n",
            "|    20|     70000|Scarlett|Software Development|\n",
            "+------+----------+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5"
      ],
      "metadata": {
        "id": "n9r1fMxenENl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ5zxCefs6j8",
        "outputId": "15f8c58a-8da7-438b-961d-30a4142e4e1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sum of Values using Accumulator: 55\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\", \"Accumulator app\")\n",
        "rdd = sc.parallelize(range(1, 11))\n",
        "\n",
        "# Define accumulator\n",
        "accumulator = sc.accumulator(0)\n",
        "def update_acc(x):\n",
        "    accumulator.add(x)\n",
        "rdd.foreach(update_acc)\n",
        "\n",
        "# Print final value of accumulator\n",
        "print(\"\\nSum of Values using Accumulator:\", accumulator.value)"
      ]
    }
  ]
}