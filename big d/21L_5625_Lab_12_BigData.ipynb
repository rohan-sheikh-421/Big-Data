{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B73yb6Suy-33",
        "outputId": "c01f3dbe-8982-45f9-baf3-52f826a4881b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=32a538076fae9731ed5c8f8ef7736cbbf166fe0a146b26395fff3edd71844bb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CORRELATION**"
      ],
      "metadata": {
        "id": "RgSf2D_l0YZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"correlation_example\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
        "    (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
        "    (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
        "    (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"features\"])\n",
        "\n",
        "r1 = Correlation.corr(df, \"features\").head()\n",
        "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
        "\n",
        "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
        "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2VuABQmzEZt",
        "outputId": "ca9006c2-72b2-456c-8123-3a5177cfb150"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation matrix:\n",
            "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
            "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
            "             [       nan,        nan, 1.        ,        nan],\n",
            "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
            "Spearman correlation matrix:\n",
            "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
            "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
            "             [       nan,        nan, 1.        ,        nan],\n",
            "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHI SQUARE**"
      ],
      "metadata": {
        "id": "ZyyOn86o0ckG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import ChiSquareTest\n",
        "\n",
        "data = [\n",
        "    (0.0, Vectors.dense(0.5, 10.0)),\n",
        "    (0.0, Vectors.dense(1.5, 20.0)),\n",
        "    (1.0, Vectors.dense(1.5, 30.0)),\n",
        "    (0.0, Vectors.dense(3.5, 30.0)),\n",
        "    (0.0, Vectors.dense(3.5, 40.0)),\n",
        "    (1.0, Vectors.dense(3.5, 40.0))\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
        "\n",
        "\n",
        "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
        "\n",
        "print(\"pValues: \" + str(r.pValues))\n",
        "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
        "print(\"statistics: \" + str(r.statistics))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG4H0gGOzRR1",
        "outputId": "656e3b45-1029-4cd0-e8c7-79a532cf2a7c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pValues: [0.6872892787909721,0.6822703303362126]\n",
            "degreesOfFreedom: [2, 3]\n",
            "statistics: [0.75,1.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUMMARIZER**"
      ],
      "metadata": {
        "id": "YDAzJbCb1M7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.stat import Summarizer\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
        "    Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))\n",
        "])\n",
        "summarizer = Summarizer.metrics(\"mean\", \"count\")\n",
        "\n",
        "# Compute statistics for multiple metrics with weight\n",
        "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
        "\n",
        "# Compute statistics for multiple metrics without weight\n",
        "df.select(summarizer.summary(df.features)).show(truncate=False)\n",
        "\n",
        "# compute statistics for single metric \"mean\" with weight\n",
        "df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
        "\n",
        "# compute statistics for single metric \"mean\" without weight\n",
        "df.select(Summarizer.mean(df.features)).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0EfrjEm0_bN",
        "outputId": "f0ea9b10-c05a-4197-9e2f-e12872fe4438"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+\n",
            "|aggregate_metrics(features, weight)|\n",
            "+-----------------------------------+\n",
            "|{[1.0,1.0,1.0], 1}                 |\n",
            "+-----------------------------------+\n",
            "\n",
            "+--------------------------------+\n",
            "|aggregate_metrics(features, 1.0)|\n",
            "+--------------------------------+\n",
            "|{[1.0,1.5,2.0], 2}              |\n",
            "+--------------------------------+\n",
            "\n",
            "+--------------+\n",
            "|mean(features)|\n",
            "+--------------+\n",
            "|[1.0,1.0,1.0] |\n",
            "+--------------+\n",
            "\n",
            "+--------------+\n",
            "|mean(features)|\n",
            "+--------------+\n",
            "|[1.0,1.5,2.0] |\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "Gm6N5era1jzG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}